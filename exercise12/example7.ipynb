{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1eedb27",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task 1. Create one pandas dataframe that combines all the data scraped from May 22, 2022 together. Drop rows with missing job titles and/or job descriptions. Use `spacy` to tokenize all the job descriptions included in the cleaned dataframe. Count how many unique tokens are there in total, and describe the distribution of token tags among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d02134",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "path = os.chdir(\"/Users/[editted]/Desktop/compSoc/data/indeed_scraped_data/job_info_data\")\n",
    "\n",
    "file_list = os.listdir(path)\n",
    "for file in file_list:\n",
    "    if \"5222022\" not in file:\n",
    "        os.remove(file)\n",
    "        \n",
    "df_concat = pd.concat([pd.read_csv(f) for f in file_list], ignore_index=True)\n",
    "cleaned_df = df_concat.dropna(subset=[\"lnks_job_title\", \"lnks_job_description\"])\n",
    "usable_df = cleaned_df.reset_index(drop=True)\n",
    "usable_df.head()\n",
    "\n",
    "Token_frew_dict = {}\n",
    "unique_tokens = set()\n",
    "token2tag = {}\n",
    "job_info=usable_df.loc[0, \"lnks_job_description\"]\n",
    "\n",
    "\n",
    "\n",
    "x = 0\n",
    "while x < len(usable_df):\n",
    "    job_info = usable_df.loc[x,\"lnks_job_description\"]\n",
    "    for token in nlp(job_info):\n",
    "        unique_tokens.add(token.text)\n",
    "        token2tag[token.text] = token.pos_\n",
    "    x +=1\n",
    "    \n",
    "\n",
    "token_df = pd.DataFrame(columns=['token', 'tag'])\n",
    "token_df['token'] = token2tag.keys()\n",
    "token_df['tag'] = token2tag.values()\n",
    "\n",
    "print('number of unique tokens in the job description:', len(token2tag))\n",
    "print('the distribution of token tags among them:')\n",
    "print(token_df['tag'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8bece",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80daa350",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Task 2. Construct a token/term frequency dictionary from the cleaned dataframe. The keys of the dictionary should be unique tokens, and the values being the frequency of the tokens in the entire corpus. Save that dictionary to a json file. What are the top 10 most common tokens according to your frequency dictionary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c367f34",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "path = os.chdir(\"/Users/[editted]/Desktop/compSoc/data/indeed_scraped_data/job_info_data\")\n",
    "\n",
    "file_list = os.listdir(path)\n",
    "for file in file_list:\n",
    "    if \"5222022\" not in file:\n",
    "        os.remove(file)\n",
    "        \n",
    "df_concat = pd.concat([pd.read_csv(f) for f in file_list], ignore_index=True)\n",
    "cleaned_df = df_concat.dropna(subset=[\"lnks_job_title\", \"lnks_job_description\"])\n",
    "usable_df = cleaned_df.reset_index(drop=True)\n",
    "\n",
    "unique_tokens = set()\n",
    "Token_freq_dict = {}\n",
    "x = 0\n",
    "while x < len(usable_df):\n",
    "    job_info = usable_df.loc[0,\"lnks_job_description\"]\n",
    "    for token in nlp(job_info):\n",
    "        unique_tokens.add(token.text)\n",
    "    for token in unique_tokens:\n",
    "        Token_freq_dict[token] = job_info.split().count(token)\n",
    "        \n",
    "\n",
    "\n",
    "print(Token_freq_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd13cac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}