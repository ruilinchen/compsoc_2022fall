{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Task 1. Integrate information from the “job_url_data” folder into one dictionary. The keys of the dictionary are individual urls scraped from the website, and the values are the earliest date that the corresponding urls were scraped. Save the results into a json file. How many unique job urls have been collected between May 17, 2022 and May 23, 2022?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "path_takehome1= \"/Users/[editted]/Documents/BC/SOCY7700/compSoc/take home 1/indeed_scraped_data/job_url_data\"\n",
    "os.chdir(path_takehome1)\n",
    "job517_v1_df=pd.read_csv('job_urls_for_parsehub_5172022_v1.csv')\n",
    "job517_v2_df=pd.read_csv('job_urls_for_parsehub_5172022_v2.csv')\n",
    "job517_v3_df=pd.read_csv('job_urls_for_parsehub_5172022_v3.csv')\n",
    "job517_v4_df=pd.read_csv('job_urls_for_parsehub_5172022_v4.csv')\n",
    "job517_v5_df=pd.read_csv('job_urls_for_parsehub_5172022_v5.csv')\n",
    "job518_v1_df=pd.read_csv('job_urls_for_parsehub_5182022_v1.csv')\n",
    "job519_v1_df=pd.read_csv('job_urls_for_parsehub_5192022_v1.csv')\n",
    "job519_v2_df=pd.read_csv('job_urls_for_parsehub_5192022_v2.csv')\n",
    "job520_v1_df=pd.read_csv('job_urls_for_parsehub_5202022_v1.csv')\n",
    "job520_v2_df=pd.read_csv('job_urls_for_parsehub_5202022_v2.csv')\n",
    "job521_v1_df=pd.read_csv('job_urls_for_parsehub_5212022_v1.csv')\n",
    "job521_v2_df=pd.read_csv('job_urls_for_parsehub_5212022_v2.csv')\n",
    "job522_v1_df=pd.read_csv('job_urls_for_parsehub_5222022_v1.csv')\n",
    "job522_v2_df=pd.read_csv('job_urls_for_parsehub_5222022_v2.csv')\n",
    "job523_v1_df=pd.read_csv('job_urls_for_parsehub_5232022_v1.csv')\n",
    "job523_v2_df=pd.read_csv('job_urls_for_parsehub_5232022_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "job517_v1_df[\"time\"]= \"05172022\"\n",
    "job517_v2_df[\"time\"]= \"05172022\"\n",
    "job517_v3_df[\"time\"]= \"05172022\"\n",
    "job517_v4_df[\"time\"]= \"05172022\"\n",
    "job517_v5_df[\"time\"]= \"05172022\"\n",
    "job518_v1_df[\"time\"]= \"05182022\"\n",
    "job519_v1_df[\"time\"]= \"05192022\"\n",
    "job519_v2_df[\"time\"]= \"05192022\"\n",
    "job520_v1_df[\"time\"]= \"05202022\"\n",
    "job520_v2_df[\"time\"]= \"05202022\"\n",
    "job521_v1_df[\"time\"]= \"05212022\"\n",
    "job521_v2_df[\"time\"]= \"05212022\"\n",
    "job522_v1_df[\"time\"]= \"05222022\"\n",
    "job522_v2_df[\"time\"]= \"05222022\"\n",
    "job523_v1_df[\"time\"]= \"05232022\"\n",
    "job523_v2_df[\"time\"]= \"05232022\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "job_df=pd.concat([job517_v1_df,job517_v2_df,job517_v3_df,job517_v4_df,job517_v5_df,job518_v1_df,job519_v1_df,job519_v2_df,job520_v1_df,job520_v2_df,job521_v1_df,job521_v2_df,job522_v1_df,job522_v2_df,job523_v1_df,job523_v2_df],ignore_index=True)\n",
    "\n",
    "job_dropdu_df=job_df.drop_duplicates(subset=['job_url'])\n",
    "job_dropdu_df.head()\n",
    "dict1=dict(zip(job_dropdu_df.job_url, job_dropdu_df.time))\n",
    "type(dict1)\n",
    "with open(\"job.json\", \"w\") as outfile:\n",
    "    json.dump(dict1,outfile)\n",
    "len(job_dropdu_df)\n",
    "job_dropdu_df.to_csv('clean_job_urls_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Task 2. Clean and integrate information from the “job_info_data” folder into one data frame. Files from this subfolder might have two different formats. Some of them are csv files, while others are json files. The columns might also be named differently. Find ways to read each of the files into pandas, drop records with missing job titles and/or missing job descriptions, and combine them into one dataframe. Lastly, drop records with duplicate job urls, and then save them into a separate csv file. How many unique jobs are there in the cleaned dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_takehome2= \"/Users/[editted]/Documents/BC/SOCY7700/compSoc/take home 1/indeed_scraped_data/job_info_data\"\n",
    "os.chdir(path_takehome2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('parsehub_5182022_v2_copy.json')\n",
    "\n",
    "df.to_csv('parsehub_5182022_v2_copy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('parsehub_5202022_v2_copy.json')\n",
    "\n",
    "df.to_csv('parsehub_5202022_v2_copy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p517_v1_df=pd.read_csv('parsehub_5172022_v1.csv')\n",
    "p517_v2_df=pd.read_csv('parsehub_5172022_v2.csv')\n",
    "p518_v1_df=pd.read_csv('parsehub_5182022_v1.csv')\n",
    "p518_v2_copy_df=pd.read_csv('parsehub_5182022_v2_copy.csv')\n",
    "p519_v1_df=pd.read_csv('parsehub_5192022_v1.csv')\n",
    "p519_v2_df=pd.read_csv('parsehub_5192022_v2.csv')\n",
    "p520_v1_df=pd.read_csv('parsehub_5202022_v1.csv')\n",
    "p520_v2_copy_df=pd.read_csv('parsehub_5202022_v2_copy.csv')\n",
    "p521_v1_df=pd.read_csv('parsehub_5212022_v1.csv')\n",
    "p521_v2_df=pd.read_csv('parsehub_5212022_v2.csv')\n",
    "p522_v1_df=pd.read_csv('parsehub_5222022_v1.csv')\n",
    "p522_v2_df=pd.read_csv('parsehub_5222022_v2.csv')\n",
    "p522_v3_df=pd.read_csv('parsehub_5222022_v3.csv')\n",
    "p522_v4_df=pd.read_csv('parsehub_5222022_v4.csv')\n",
    "p522_v5_df=pd.read_csv('parsehub_5222022_v5.csv')\n",
    "p522_v6_df=pd.read_csv('parsehub_5222022_v6.csv')\n",
    "p523_v1_df=pd.read_csv('parsehub_5232022_v1.csv')\n",
    "p523_v2_df=pd.read_csv('parsehub_5222022_v2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['link', 'job_title', 'company', 'company_url', 'company_location',\n",
      "       'job_description'],\n",
      "      dtype='object')\n",
      "Index(['lnks_link', 'lnks_job_title', 'lnks_company', 'lnks_company_url',\n",
      "       'lnks_company_location', 'lnks_job_description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(p520_v2_copy_df.columns)\n",
    "print(p522_v2_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lnks_link', 'lnks_job_title', 'lnks_company', 'lnks_company_url',\n",
      "       'lnks_company_location', 'lnks_job_description'],\n",
      "      dtype='object')\n",
      "Index(['lnks_link', 'lnks_job_title', 'lnks_company', 'lnks_company_url',\n",
      "       'lnks_company_location', 'lnks_job_description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "p520_v2_copy_df.rename(columns={'link':'lnks_link', 'job_title':'lnks_job_title', 'company':'lnks_company', 'company_url':'lnks_company_url','company_location':'lnks_company_location','job_description':'lnks_job_description'},inplace = True)\n",
    "print(p520_v2_copy_df.columns)\n",
    "p518_v2_copy_df.rename(columns={'link':'lnks_link', 'job_title':'lnks_job_title', 'company':'lnks_company', 'company_url':'lnks_company_url','company_location':'lnks_company_location','job_description':'lnks_job_description'},inplace = True)\n",
    "print(p518_v2_copy_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44488, 12)\n"
     ]
    }
   ],
   "source": [
    "job_info_df=pd.concat([p517_v1_df,p517_v2_df,p518_v1_df,p518_v2_copy_df,p519_v1_df,p519_v2_df,p520_v1_df,p520_v2_copy_df,p521_v1_df,p521_v2_df,p522_v1_df,p522_v2_df,p522_v3_df,p522_v4_df,p522_v5_df,p522_v6_df,p523_v1_df,p523_v2_df],ignore_index=True)\n",
    "\n",
    "print(job_info_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19578, 12)\n"
     ]
    }
   ],
   "source": [
    "job_info_nona_df=job_info_df.dropna(how='any',subset=['lnks_job_title', 'lnks_job_description'])\n",
    "print(job_info_nona_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15526, 12)\n"
     ]
    }
   ],
   "source": [
    "clean_job_info_df=job_info_nona_df.drop_duplicates(subset=['lnks_company_url'])\n",
    "print(clean_job_info_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15526, 12)\n"
     ]
    }
   ],
   "source": [
    "clean_job_info_df.to_csv('clean_job_info_data.csv', index=False)\n",
    "print(clean_job_info_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### There are 15526 unique jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15526, 12)\n",
      "(15525, 12)\n"
     ]
    }
   ],
   "source": [
    "# another way to clean data\n",
    "clean_job_info_df=pd.read_csv('clean_job_info_data.csv')\n",
    "print(clean_job_info_df.shape)\n",
    "clean_nona_df=clean_job_info_df.dropna(how='any',subset=['lnks_link', 'lnks_job_title', 'lnks_company', 'lnks_company_url',\n",
    "       'lnks_company_location', 'lnks_job_description'])\n",
    "print(clean_nona_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15525, 12)\n"
     ]
    }
   ],
   "source": [
    "finalclean_job_info_df=clean_nona_df.drop_duplicates(subset=['lnks_company_url'])\n",
    "print(finalclean_job_info_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Task 3. Merge between “job_url_data” and “job_info_data”. What is the percentage of jobs that can be matched between these two data sources? How are the missing data (unmatched job urls) distributed by date? What about matched job urls? How many complete job listings were we able to collect each day? How would you interpret this result with respect to data quality? Does it mean that our data collection strategy is flawed and thus introduces non-random sampling biases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "path_takehome1_3= \"/Users/apple/Documents/BC/SOCY7700/compSoc/take home 1/indeed_scraped_data\"\n",
    "os.chdir(path_takehome1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clean_job_info_df=pd.read_csv('clean_job_info_data.csv')\n",
    "clean_job_urls_df=pd.read_csv('clean_job_urls_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lnks_link', 'lnks_job_title', 'lnks_company', 'lnks_company_url',\n",
      "       'lnks_company_location', 'lnks_job_description', 'link', 'job_title',\n",
      "       'company', 'company_url', 'company_location', 'job_description'],\n",
      "      dtype='object')\n",
      "Index(['job_url', 'time'], dtype='object')\n",
      "(15526, 12)\n",
      "(21260, 2)\n"
     ]
    }
   ],
   "source": [
    "print(clean_job_info_df.columns)\n",
    "print(clean_job_urls_df.columns)\n",
    "print(clean_job_info_df.shape)\n",
    "print(clean_job_urls_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lnks_link', 'time'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "clean_job_urls_df.rename(columns={'job_url':'lnks_link'},inplace = True)\n",
    "print(clean_job_urls_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lnks_link', 'time', 'lnks_job_title', 'lnks_company',\n",
      "       'lnks_company_url', 'lnks_company_location', 'lnks_job_description',\n",
      "       'link', 'job_title', 'company', 'company_url', 'company_location',\n",
      "       'job_description'],\n",
      "      dtype='object')\n",
      "(15526, 13)\n"
     ]
    }
   ],
   "source": [
    "jobs_df=clean_job_urls_df.merge(clean_job_info_df,how='inner', on='lnks_link')\n",
    "print(jobs_df.columns)\n",
    "print(jobs_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lnks_link', 'time', 'lnks_job_title', 'lnks_company',\n",
      "       'lnks_company_url', 'lnks_company_location', 'lnks_job_description',\n",
      "       'link', 'job_title', 'company', 'company_url', 'company_location',\n",
      "       'job_description'],\n",
      "      dtype='object')\n",
      "(24222, 13)\n"
     ]
    }
   ],
   "source": [
    "totaljobs_df=clean_job_urls_df.merge(clean_job_info_df,how='outer', on='lnks_link')\n",
    "print(totaljobs_df.columns)\n",
    "print(totaljobs_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6409875319957063"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of jobs that can be matched between these two data \n",
    "len(jobs_df)/len(totaljobs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 64.1% jobs could be matched between these two data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      time  lnks_link\n",
      "0  5172022       2289\n",
      "1  5182022        743\n",
      "2  5192022       4267\n",
      "3  5202022       5436\n",
      "4  5212022       2272\n",
      "5  5222022        519\n"
     ]
    }
   ],
   "source": [
    "#How many complete job listings were we able to collect each day?\n",
    "jobs_count_by_day=jobs_df.groupby('time').agg('lnks_link').count().reset_index()\n",
    "print(jobs_count_by_day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### How would you interpret this result with respect to data quality? Does it mean that our data collection strategy is flawed and thus introduces non-random sampling biases?¶\n",
    "* No complete job listings were collected on 5/23. Most jobs were found on 5/19 and 5/20. Less jobs were found on 5/22 and 8/18.\n",
    "* I don't think something is wrong with the data collection. But I do suggest to do more reserch about what happened on 5/19 and 5/20. What are speical on 5/18 and 5/22. Or do more data collection to find if the trend between 5/17 to 5/23 is common. If it's common on other days, then nothing is wrong. Another possibilitiy is that most jobs found on 5/22 and 5/23 were same as previous days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      time  lnks_link\n",
      "0  5172022       5316\n",
      "1  5182022       1422\n",
      "2  5192022       4838\n",
      "3  5202022       5763\n",
      "4  5212022       2680\n",
      "5  5222022        780\n",
      "6  5232022       3423\n"
     ]
    }
   ],
   "source": [
    "# How are the missig data distrabuted by date\n",
    "totaljobs_count_by_day=totaljobs_df.groupby('time').agg('lnks_link').count().reset_index()\n",
    "print(totaljobs_count_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3027, 679, 571, 327, 408, 261, 3423)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5316-2289, 1422-743,4838-4267,5763-5436,2680-2272,780-519,3423-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### There are most missing data on 5/17 and 5/23, which means there are most missing data on the first day and second. It seems for the first day, the job information is not complete enough, and as time goes on, job information becomes complete. But after several days, more there are more missing data again. Maybe too much repetead job information is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}