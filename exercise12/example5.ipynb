{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Task 1\n",
    "Create one pandas dataframe that combines all the data scraped from May 22, 2022 together. Drop rows with missing job titles and/or job descriptions. Use `spacy` to tokenize all the job descriptions included in the cleaned dataframe. Count how many unique tokens are there in total, and describe the distribution of token tags among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "os.chdir('/Users/[editted]/Desktop/compsoc/data/indeed_scraped_data/job_info_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ml/2z7dml2d3vn2rqmr8h2sw_n40000gn/T/ipykernel_2648/351113638.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  May22_all = May22_all.append(read_file, ignore_index=True)\n",
      "/var/folders/ml/2z7dml2d3vn2rqmr8h2sw_n40000gn/T/ipykernel_2648/351113638.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  May22_all = May22_all.append(read_file, ignore_index=True)\n",
      "/var/folders/ml/2z7dml2d3vn2rqmr8h2sw_n40000gn/T/ipykernel_2648/351113638.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  May22_all = May22_all.append(read_file, ignore_index=True)\n",
      "/var/folders/ml/2z7dml2d3vn2rqmr8h2sw_n40000gn/T/ipykernel_2648/351113638.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  May22_all = May22_all.append(read_file, ignore_index=True)\n",
      "/var/folders/ml/2z7dml2d3vn2rqmr8h2sw_n40000gn/T/ipykernel_2648/351113638.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  May22_all = May22_all.append(read_file, ignore_index=True)\n",
      "/var/folders/ml/2z7dml2d3vn2rqmr8h2sw_n40000gn/T/ipykernel_2648/351113638.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  May22_all = May22_all.append(read_file, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Combine data from May 22 and drop rows with missing values\n",
    "May22_all = pd.DataFrame()\n",
    "for file in glob.glob('*.csv'):\n",
    "    if file[9:16] == '5222022':\n",
    "        #print(file)\n",
    "        read_file = pd.read_csv(file)\n",
    "        May22_all = May22_all.append(read_file, ignore_index=True)\n",
    "# May22_all\n",
    "cleaned_May22 = May22_all.dropna(subset=['lnks_job_title', 'lnks_job_description'])\n",
    "#cleaned_May22\n",
    "cleaned_May22.to_csv('May22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4803/4803 [19:20<00:00,  4.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens in the job description: 39150\n",
      "the distribution of token tas among them:\n",
      "PROPN    15204\n",
      "NOUN     11208\n",
      "VERB      4613\n",
      "ADJ       3267\n",
      "NUM       3206\n",
      "ADV        819\n",
      "X          198\n",
      "ADP        142\n",
      "PRON       112\n",
      "PUNCT       83\n",
      "SPACE       77\n",
      "AUX         65\n",
      "SCONJ       48\n",
      "DET         45\n",
      "INTJ        22\n",
      "CCONJ       18\n",
      "PART        14\n",
      "SYM          9\n",
      "Name: tag, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the job descriptions and tokenize them\n",
    "May22 = pd.read_csv('May22.csv', low_memory=False)\n",
    "print('')\n",
    "describe_token = {}\n",
    "for i in tqdm(range(len(May22))):\n",
    "    job_description = May22.loc[i, 'lnks_job_description']\n",
    "    for token in nlp(job_description):\n",
    "        describe_token[token.text] = token.pos_\n",
    "    token_df = pd.DataFrame(columns=['token', 'tag'])\n",
    "    token_df['token'] = describe_token.keys()\n",
    "    token_df['tag'] = describe_token.values()\n",
    "# token_df\n",
    "\n",
    "print('number of unique tokens in the job description:', len(describe_token))\n",
    "print('the distribution of token tas among them:')\n",
    "print(token_df['tag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens in the job description: 39150\n"
     ]
    }
   ],
   "source": [
    "#Double Check\n",
    "# unique_tokens =set()\n",
    "# for i in range(len(May22)):\n",
    "#     job_description = May22.loc[i, 'lnks_job_description']\n",
    "#     for token in nlp(job_description):\n",
    "#         unique_tokens.add(token.text)\n",
    "#\n",
    "# print('number of unique tokens in the job description:', len(unique_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Task 1 Conclusion**\n",
    "There are 39150 unique tokens in total. The distribution of token tags are printed as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Task 2\n",
    "Construct a token/term frequency dictionary from the cleaned dataframe. The keys of the dictionary should be unique tokens, and the values being the frequency of the tokens in the entire corpus. Save that dictionary to a json file. What are the top 10 most common tokens according to your frequency dictionary?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4803/4803 [13:10<00:00,  6.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "May22 = pd.read_csv('May22.csv', low_memory=False)\n",
    "print('')\n",
    "counts = Counter()\n",
    "for i in tqdm(range(len(May22))):\n",
    "    job_description = May22.loc[i, 'lnks_job_description']\n",
    "    for token in nlp(job_description):\n",
    "        counts[token.orth_] += 1\n",
    "token_freq = dict(counts)\n",
    "# token_freq\n",
    "with open ('522_token_frequency.json','w') as g:\n",
    "    json.dump(token_freq, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  token  frequency\n",
      "0     ,     254381\n",
      "1   and     240686\n",
      "2    \\n     227694\n",
      "3     .     172173\n",
      "4    to     119440\n",
      "5   the     100408\n",
      "6    of      97891\n",
      "7    in      60798\n",
      "8  with      50655\n",
      "9     a      49613\n"
     ]
    }
   ],
   "source": [
    "#Get the first ten rows\n",
    "freq_df = pd.DataFrame(token_freq.items())\n",
    "freq_df = freq_df.rename(columns={0: 'token'})\n",
    "freq_df = freq_df.rename(columns={1: 'frequency'})\n",
    "freq_df = freq_df.sort_values(by='frequency', ascending=False, ignore_index=True)\n",
    "result = freq_df.head(10)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Task 2 Conclusion**\n",
    "According to the frequency dictionary (visualized by converting to dataframe), the 10 most common tokens in the entire corpus are: \",\", \"and\", \"\\n\", \".\", \"to\", \"the\", \"of\", \"in\", \"with\" and \"a\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}