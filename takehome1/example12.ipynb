{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13cde24a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Take Home Assignment #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403eeabd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93818f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Integrate information from the “job_url_data” folder into one dictionary. The keys of the dictionary are individual urls scraped from the website, and the values are the earliest date that the corresponding urls were scraped. Save the results into a json file. How many unique job urls have been collected between May 17, 2022 and May 23, 2022?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1b4166",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('/Users/[editted]/Desktop/SOCDATA/Take_home_1/indeed_scraped_data/job_url_data')\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "\n",
    "###Adding column in dataset\n",
    "with open('url_output.csv','w',newline='') as f_output:\n",
    "    url_output=csv.writer(f_output)\n",
    "    for name in glob.glob('*.csv'):\n",
    "        if len(name)>15:\n",
    "            with open(name,newline='') as inputs:\n",
    "                url_input = csv.reader(inputs)\n",
    "                for element in url_input:\n",
    "                    element.insert(0,name[22:29]) # name[22:29]: where the date lies\n",
    "                    url_output.writerow(element)\n",
    "\n",
    "url_total = pd.read_csv('url_output.csv', low_memory=False, header=None)\n",
    "url_total = url_total[url_total[1].str.contains(\"job_url\") == False]\n",
    "\n",
    "url_total# Checking the dataset\n",
    "url_total = url_total.rename(columns={0: 'Date'})\n",
    "url_total = url_total.rename(columns={1: 'job_url'})\n",
    "url_total = url_total.groupby('job_url',as_index=False).min()\n",
    "\n",
    "url_total = url_total.drop_duplicates()\n",
    "url_total.shape\n",
    "##Analysis: There are 21260 unique job urls\n",
    "\n",
    "###Making doctionary\n",
    "url_total_d = url_total\n",
    "url_total_d.set_index('job_url',inplace=True)\n",
    "url_d = url_total_d.to_dict()['Date']\n",
    "url_d #Checking dictionary\n",
    "\n",
    "###Converting to json\n",
    "with open ('url_output.json','w') as json_f:\n",
    "    json.dump(url_d,json_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68fa13",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148352c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Clean and integrate information from the “job_info_data” folder into one data frame. Files from this subfolder might have two different formats. Some of them are csv files, while others are json files. The columns might also be named differently. Find ways to read each of the files into pandas, drop records with missing job titles and/or missing job descriptions, and combine them into one dataframe. Lastly, drop records with duplicate job urls, and then save them into a separate csv file. How many unique jobs are there in the cleaned dataframe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5280d0f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lnks_link', 'lnks_job_title', 'lnks_company', 'lnks_company_url',\n",
      "       'lnks_company_location', 'lnks_job_description'],\n",
      "      dtype='object')\n",
      "Index(['lnks_link', 'lnks_job_title', 'lnks_company', 'lnks_company_url',\n",
      "       'lnks_company_location', 'lnks_job_description'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16303, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/[editted]/Desktop/SOCDATA/Take_home_1/indeed_scraped_data/job_info_data')\n",
    "\n",
    "###Converting Jason\n",
    "##json_1\n",
    "json_1 = pd.read_json('parsehub_5182022_v2.json')\n",
    "d_1 = pd.DataFrame.from_dict(json_1)\n",
    "lnk_d1 = d_1['lnks'].apply(pd.Series)\n",
    "##json_2\n",
    "json_2 = pd.read_json('parsehub_5202022_v2.json')\n",
    "d_2 = pd.DataFrame.from_dict(json_2)\n",
    "lnk_d2 = d_2['lnks'].apply(pd.Series)\n",
    "##Change column names, Check names, and store the dataset\n",
    "lnk_d1 = lnk_d1.rename(columns={'link':'lnks_link','job_title':'lnks_job_title','company':'lnks_company','company_url':'lnks_company_url','company_location':'lnks_company_location','job_description':'lnks_job_description'})\n",
    "print(lnk_d1.columns)\n",
    "lnk_d1.to_csv('parsehub_5182022_v2.csv')\n",
    "lnk_d2 = lnk_d2.rename(columns={'link':'lnks_link','job_title':'lnks_job_title','company':'lnks_company','company_url':'lnks_company_url','company_location':'lnks_company_location','job_description':'lnks_job_description'})\n",
    "print(lnk_d2.columns)\n",
    "lnk_d2.to_csv('parsehub_5202022_v2.csv')\n",
    "\n",
    "###Merge all the data set in the info folder\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "all_filenames\n",
    "#combine all files in the list\n",
    "combined_csv_info = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv_info.shape # Total of 43877 columns\n",
    "# combined_csv_info\n",
    "\n",
    "##Data Cleaning\n",
    "#Drop NAs\n",
    "combined_csv_info_new = combined_csv_info.dropna(subset=['lnks_job_title','lnks_job_description'])\n",
    "##Drop Duplicates\n",
    "combined_csv_info_final = combined_csv_info_new.drop_duplicates(subset=['lnks_link'])\n",
    "combined_csv_info_final.shape\n",
    "\n",
    "###There are 16303 unique job urls\n",
    "\n",
    "\n",
    "###Write csv\n",
    "combined_csv_info_final.to_csv( \"combined_csv_info_final.csv\", index=False, encoding='utf-8-sig')\n",
    "# combined_csv_info_final.columns\n",
    "combined_csv_info_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232286cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894e0ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Merge between “job_url_data” and “job_info_data”. What is the percentage of jobs that can be matched between these two data sources? How are the missing data (unmatched job urls) distributed by date? What about matched job urls? How many complete job listings were we able to collect each day? How would you interpret this result with respect to data quality? Does it mean that our data collection strategy is flawed and thus introduces non-random sampling biases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221eecbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5172022: 3905,\n",
       " 5202022: 4503,\n",
       " 5192022: 3790,\n",
       " 5182022: 1324,\n",
       " 5212022: 2113,\n",
       " 5222022: 668}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Merge Data Set\n",
    "##Getting the data set\n",
    "os.chdir('/Users/[editted]/Desktop/SOCDATA/Take_home_1/indeed_scraped_data/job_url_data')\n",
    "##Open data sets\n",
    "url_total = pd.read_csv('url_output.csv', low_memory=False, header=None)\n",
    "url_total = url_total[url_total[1].str.contains(\"job_url\") == False]\n",
    "url_total = url_total.rename(columns={0: 'Date'})\n",
    "url_total = url_total.rename(columns={1: 'job_url'})\n",
    "url_total = url_total.groupby('job_url',as_index=False).min()\n",
    "\n",
    "url_total = url_total.drop_duplicates()\n",
    "url_total\n",
    "##Checking columns\n",
    "# print(url_total.columns)\n",
    "# print(combined_csv_info_final.columns)\n",
    "\n",
    "##Merge Data set\n",
    "total_final = url_total.merge(combined_csv_info_final, left_on='job_url',right_on='lnks_link')\n",
    "total_final = total_final.drop_duplicates()\n",
    "total_final.shape #There are total 16303 data entries\n",
    "\n",
    "###Percentage of merging successfully\n",
    "#16303/(16303+21260)\n",
    "#There are 43.4% files merged successfully \n",
    "\n",
    "###How many complete job listings were we able to collect each day?\n",
    "total_final = total_final.dropna(subset=['lnks_job_title','lnks_job_description'])\n",
    "#There are 16303 complete job listings that can be colleced, meaning no missing titles and descriptions.\n",
    "\n",
    "###Daily Quality and Effectiveness of data collection process\n",
    "date_d = {}\n",
    "\n",
    "for element in total_final['Date']:\n",
    "    if element not in date_d:\n",
    "        date_d[element] = 1\n",
    "    else:\n",
    "        date_d[element] +=1\n",
    "date_d\n",
    "##Finding: i think the data quality is pretty correct because most of the jobs are \n",
    "#          found on 5/17, 5/18, 5/19 which are all week days, \n",
    "#          since there are more jobs appearing during week days than weekends\n",
    "\n",
    "##Since this is a job mining project, I do not necesaarily think random selecting is required.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}