{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314af958-6ec9-456f-bf28-a4e8ad774165",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Take-Home 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e020f-5bd5-4758-bcb7-8230bd1be285",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Task 1.\n",
    "Integrate information from the “job_url_data” folder into one dictionary. The keys of the dictionary are individual urls scraped from the website, and the values are the earliest date that the corresponding urls were scraped. Save the results into a json file. How many unique job urls have been collected between May 17, 2022 and May 23, 2022?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a9b875d3-b4df-4a95-aaac-645fd13a8e4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job_urls_for_parsehub_5232022_v2.csv', 'job_urls_for_parsehub_5232022_v1.csv', 'job_urls_for_parsehub_5222022_v2.csv', 'job_urls_for_parsehub_5222022_v1.csv', 'job_urls_for_parsehub_5212022_v2.csv', 'job_urls_for_parsehub_5212022_v1.csv', 'job_urls_for_parsehub_5202022_v2.csv', 'job_urls_for_parsehub_5202022_v1.csv', 'job_urls_for_parsehub_5192022_v2.csv', 'job_urls_for_parsehub_5192022_v1.csv', 'job_urls_for_parsehub_5182022_v1.csv', 'job_urls_for_parsehub_5172022_v5.csv', 'job_urls_for_parsehub_5172022_v4.csv', 'job_urls_for_parsehub_5172022_v3.csv', 'job_urls_for_parsehub_5172022_v2.csv', 'job_urls_for_parsehub_5172022_v1.csv']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.getcwd()\n",
    "path='/Users/[editted]/Dropbox/work/compsoc/dataset/indeed_scraped_data/job_url_data'\n",
    "os.chdir(path)\n",
    "file_list=[i for i in os.listdir(path) if i.endswith('.csv')]\n",
    "file_list.sort(reverse=True)\n",
    "print(file_list)\n",
    "\n",
    "file_dict={}\n",
    "for item in file_list:\n",
    "    with open(item, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('http'):\n",
    "                linec=line.replace('\\n', '') # url is a key\n",
    "                date=item.split('_')[4] # date is a value\n",
    "                file_dict[linec]=date \n",
    "                # {'url': 'date'} \n",
    "                # as the file on May, 23 are input to the dictionary first, the date value on May, 17 remains as the value of the dictionary. \n",
    "                # Therefore, the duplicated values on the keys are replaced to the earliest date.\n",
    "\n",
    "with open(\"/Users/[editted]/Dropbox/work/compsoc/dataset/indeed_scraped_data/job_urls_json.json\", \"w\") as f:\n",
    "    json.dump(file_dict, f) ## There are 21034 job urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c80145b1-7fa2-4b0d-a73b-42cd5e4b170b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Diagnostic 1: convert Dictionary to Dataframe and check the data structure.\n",
    "# file_pd=pd.DataFrame(file_dict.items(), columns=['lnks_link', 'date'])\n",
    "# file_pd.shape\n",
    "\n",
    "# # Diagnostic 2: DataFrame List Version to check the dictionary is properly shaped by using lists to arrange the data.\n",
    "# all_list=[]\n",
    "# na_list=[]\n",
    "# for item in file_list:\n",
    "#     item_list=[]\n",
    "#     datelist=[]\n",
    "#     i=0\n",
    "#     with open(item, 'r') as f:\n",
    "#         for line in f:\n",
    "#             if line.startswith('http'):\n",
    "#                 i+=1\n",
    "#                 line_n=i\n",
    "#                 linec=line.replace('\\n', '')\n",
    "#                 date=item.split('_')[4]\n",
    "#                 item_list.append(linec)\n",
    "#                 datelist.append(date)\n",
    "#         na_list.append(line_n)\n",
    "#         print(item, \"n=\", line_n)\n",
    "#         all_list.append({'lnks_link': item_list, 'date': date})\n",
    "# print(sum(na_list))    # total amount of urls is 60403\n",
    "\n",
    "# fall=pd.DataFrame()\n",
    "# for i in range(0, len(all_list)):\n",
    "#     df_i=pd.DataFrame(all_list[i])\n",
    "#     fall=fall.append(df_i, ignore_index=True)   \n",
    "# fall.date=pd.to_datetime(fall.date, format='%m%d%Y')\n",
    "# fall.sort_values('date', inplace=True)\n",
    "# fall=fall.reset_index(drop=True)\n",
    "# fall['dup']=fall.duplicated(subset='lnks_link', keep='first')\n",
    "# fall=fall[fall.dup==False].drop(columns='dup',axis=1).reset_index(drop=True)\n",
    "# fall.to_csv('/Users/gohtk/Dropbox/work/compsoc/dataset/indeed_scraped_data/job_urls_cleaned.csv', index=False)\n",
    "# fall # after cleaning the duplicates, n=21034"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435c8e0-3a6f-410f-bab0-b47fb523c67d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Task 2. \n",
    "Clean and integrate information from the “job_info_data” folder into one data frame. Files from this subfolder might have two different formats. Some of them are csv files, while others are json files. The columns might also be named differently. Find ways to read each of the files into pandas, drop records with missing job titles and/or missing job descriptions, and combine them into one dataframe. Lastly, drop records with duplicate job urls, and then save them into a separate csv file. How many unique jobs are there in the cleaned dataframe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "da72c320-79a3-4cea-b05f-ae76f5186b9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parsehub_5232022_v2.csv', 'parsehub_5232022_v1.csv', 'parsehub_5222022_v6.csv', 'parsehub_5222022_v5.csv', 'parsehub_5222022_v4.csv', 'parsehub_5222022_v3.csv', 'parsehub_5222022_v2.csv', 'parsehub_5222022_v1.csv', 'parsehub_5212022_v2.csv', 'parsehub_5212022_v1.csv', 'parsehub_5202022_v2.json', 'parsehub_5202022_v1.csv', 'parsehub_5192022_v2.csv', 'parsehub_5192022_v1.csv', 'parsehub_5182022_v2.json', 'parsehub_5182022_v1.csv', 'parsehub_5172022_v2.csv', 'parsehub_5172022_v1.csv']\n",
      "parsehub_5202022_v2.json (4857, 1) Index(['lnks_link'], dtype='object')\n",
      "parsehub_5182022_v2.json (2833, 1) Index(['lnks_link'], dtype='object')\n",
      "                                           lnks_link  \\\n",
      "0  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
      "1  https://www.indeed.com/rc/clk?jk=2e84a8f5beb07...   \n",
      "2  https://www.indeed.com/rc/clk?jk=30d7f8247c497...   \n",
      "3  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
      "4  https://www.indeed.com/rc/clk?jk=ee370349db8d8...   \n",
      "\n",
      "                             lnks_job_title                 lnks_company  \\\n",
      "0               Clinical Nurse - Ambulatory  Phoenix Children's Hospital   \n",
      "1                        Restaurant Manager     Golden State Jacks, Inc.   \n",
      "2                             RX TechBE OCR                   CVS Health   \n",
      "3  Research Regulatory Coordinator (REMOTE)                     Actalent   \n",
      "4               Pharmacy Operations Manager                    WALGREENS   \n",
      "\n",
      "                                    lnks_company_url    lnks_company_location  \\\n",
      "0  https://www.indeed.com/cmp/Phoenix-Children's-...       New York, NY 10261   \n",
      "1  https://www.indeed.com/cmp/Golden-State-Jacks,...  Scotts Valley, CA 95066   \n",
      "2  https://www.indeed.com/cmp/CVS-Health?campaign...              Tukwila, WA   \n",
      "3  https://www.indeed.com/cmp/Actalent?campaignid...         Dallas, TX 75201   \n",
      "4  https://www.indeed.com/cmp/Walgreens?campaigni...         Toledo, OH 43605   \n",
      "\n",
      "                                lnks_job_description  \n",
      "0  Position Summary\\nPosting Note: PCH is currenl...  \n",
      "1  Position Summary\\nResponsible for managing the...  \n",
      "2  Pharmacy Technicians take important steps to e...  \n",
      "3  Equivalent Experience\\nDescription:\\nThe Clini...  \n",
      "4  Where state and federal laws/regulations allow...  \n"
     ]
    }
   ],
   "source": [
    "path2='/Users/[editted]/Dropbox/work/compsoc/dataset/indeed_scraped_data/job_info_data'\n",
    "os.chdir(path2)\n",
    "file_list=[i for i in os.listdir(path2) if i.endswith(('.csv', '.json'))]\n",
    "file_list.sort(reverse=True)\n",
    "print(file_list)\n",
    "\n",
    "info_pd=pd.DataFrame()\n",
    "\n",
    "for item in file_list:\n",
    "    if item.endswith('.csv'):\n",
    "        c=pd.read_csv(item)\n",
    "        info_pd=pd.concat([info_pd, c])\n",
    "    elif item.endswith('.json'):\n",
    "        j = pd.read_json(item)\n",
    "        j['lnks_link']=None\n",
    "        for i in range(0, len(j)):\n",
    "            for v, k in j.lnks[i].items():\n",
    "                j.lnks_link[i]=k\n",
    "        j.lnks_link=j.lnks_link.replace(r'', np.NaN) \n",
    "        j=j.dropna().drop(['lnks'], axis=1) \n",
    "        print(item, j.shape, j.columns)\n",
    "        info_pd=pd.concat([info_pd, j])\n",
    "info_pd=info_pd.dropna(subset=['lnks_job_title', 'lnks_job_description']).drop_duplicates(subset='lnks_link').reset_index(drop=True)\n",
    "print(info_pd.head())\n",
    "info_pd.to_csv('/Users/[editted]/Dropbox/work/compsoc/dataset/indeed_scraped_data/job_info_cleaned.csv', index=False)\n",
    "\n",
    "# n=14200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e2f2f-baf8-42e8-a1e3-fd5af492f433",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Task 3. \n",
    "Merge between “job_url_data” and “job_info_data”. What is the percentage of jobs that can be matched between these two data sources? How are the missing data (unmatched job urls) distributed by date? What about matched job urls? How many complete job listings were we able to collect each day? How would you interpret this result with respect to data quality? Does it mean that our data collection strategy is flawed and thus introduces non-random sampling biases?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8a855b7d-9d0a-488e-ab37-8202f2751938",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/[editted]/Dropbox/work/compsoc/dataset/indeed_scraped_data')\n",
    "\n",
    "with open('job_urls_json.json', 'r') as j:\n",
    "    juj_json=json.load(j)\n",
    "juj=pd.DataFrame.from_dict(juj_json, orient='index').reset_index().rename({'index': 'lnks_link', 0: 'date'}, axis=1)\n",
    "jic=pd.read_csv('job_info_cleaned.csv')\n",
    "job_urls_merge = pd.merge(juj, jic, how='left', on='lnks_link').sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a5ad62f9-ea8e-49af-878d-ed1cc88e81d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    14050\n",
      "True      6984\n",
      "Name: lnks_job_title, dtype: int64\n",
      "66.79661500427879\n"
     ]
    }
   ],
   "source": [
    "# What is the percentage of jobs that can be matched between these two data sources?\n",
    "print(job_urls_merge.lnks_job_title.isna().value_counts())\n",
    "tf=job_urls_merge.lnks_job_title.isna().value_counts().to_list()\n",
    "matched=tf[0]\n",
    "unmatched=tf[1]\n",
    "print(matched/(matched+unmatched)*100)\n",
    "# the rate of matched jobs to the merged data of \"job_url_data\" and \"job_info_data\" is 66.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "98cae3b0-c598-4e3c-bac7-5117ac51d101",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>job_counts</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Matched</td>\n",
       "      <td>2610</td>\n",
       "      <td>5172022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matched</td>\n",
       "      <td>672</td>\n",
       "      <td>5182022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Matched</td>\n",
       "      <td>3560</td>\n",
       "      <td>5192022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matched</td>\n",
       "      <td>4445</td>\n",
       "      <td>5202022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matched</td>\n",
       "      <td>2098</td>\n",
       "      <td>5212022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Matched</td>\n",
       "      <td>665</td>\n",
       "      <td>5222022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not Matched</td>\n",
       "      <td>2607</td>\n",
       "      <td>5172022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Not Matched</td>\n",
       "      <td>630</td>\n",
       "      <td>5182022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Not Matched</td>\n",
       "      <td>193</td>\n",
       "      <td>5192022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Not Matched</td>\n",
       "      <td>20</td>\n",
       "      <td>5202022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Not Matched</td>\n",
       "      <td>41</td>\n",
       "      <td>5212022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Not Matched</td>\n",
       "      <td>112</td>\n",
       "      <td>5222022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Not Matched</td>\n",
       "      <td>3381</td>\n",
       "      <td>5232022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         result  job_counts     date\n",
       "0       Matched        2610  5172022\n",
       "1       Matched         672  5182022\n",
       "2       Matched        3560  5192022\n",
       "3       Matched        4445  5202022\n",
       "4       Matched        2098  5212022\n",
       "5       Matched         665  5222022\n",
       "6   Not Matched        2607  5172022\n",
       "7   Not Matched         630  5182022\n",
       "8   Not Matched         193  5192022\n",
       "9   Not Matched          20  5202022\n",
       "10  Not Matched          41  5212022\n",
       "11  Not Matched         112  5222022\n",
       "12  Not Matched        3381  5232022"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How are the missing data (unmatched job urls) distributed by date? What about matched job urls?\n",
    "datelist=pd.DataFrame(job_urls_merge.date.drop_duplicates()).reset_index().date\n",
    "job_d=pd.DataFrame()\n",
    "\n",
    "for day in datelist:\n",
    "    job_date=job_urls_merge.loc[job_urls_merge.date==day]\n",
    "    job_dateday=pd.DataFrame(job_date.lnks_job_title.isna().value_counts()).reset_index().rename({'index': 'result', 'lnks_job_title': 'job_counts'}, axis=1)\n",
    "    job_dateday['date']=day\n",
    "    job_d=pd.concat([job_d,job_dateday])\n",
    "\n",
    "job_d.result=job_d.result.map({True: 'True', False: 'False'}).replace('True', 'Not Matched').replace('False', 'Matched')\n",
    "job_d=job_d.sort_values(by=['result']).reset_index(drop=True) \n",
    "job_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4c69316d-e78a-4020-a1d4-e56e6976cfe9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>total job listing</th>\n",
       "      <th>complete job listing</th>\n",
       "      <th>matched_job_counts</th>\n",
       "      <th>unmatched_job_counts</th>\n",
       "      <th>data quality_complete</th>\n",
       "      <th>data quality_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5172022</td>\n",
       "      <td>5217</td>\n",
       "      <td>2208.0</td>\n",
       "      <td>2610.0</td>\n",
       "      <td>2607</td>\n",
       "      <td>42.323174</td>\n",
       "      <td>50.028752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5182022</td>\n",
       "      <td>1302</td>\n",
       "      <td>634.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>630</td>\n",
       "      <td>48.694316</td>\n",
       "      <td>51.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5192022</td>\n",
       "      <td>3753</td>\n",
       "      <td>3174.0</td>\n",
       "      <td>3560.0</td>\n",
       "      <td>193</td>\n",
       "      <td>84.572342</td>\n",
       "      <td>94.857447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5202022</td>\n",
       "      <td>4465</td>\n",
       "      <td>4130.0</td>\n",
       "      <td>4445.0</td>\n",
       "      <td>20</td>\n",
       "      <td>92.497200</td>\n",
       "      <td>99.552072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5212022</td>\n",
       "      <td>2139</td>\n",
       "      <td>1732.0</td>\n",
       "      <td>2098.0</td>\n",
       "      <td>41</td>\n",
       "      <td>80.972417</td>\n",
       "      <td>98.083216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5222022</td>\n",
       "      <td>777</td>\n",
       "      <td>539.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>112</td>\n",
       "      <td>69.369369</td>\n",
       "      <td>85.585586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5232022</td>\n",
       "      <td>3381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  total job listing  complete job listing  matched_job_counts  \\\n",
       "0  5172022               5217                2208.0              2610.0   \n",
       "1  5182022               1302                 634.0               672.0   \n",
       "2  5192022               3753                3174.0              3560.0   \n",
       "3  5202022               4465                4130.0              4445.0   \n",
       "4  5212022               2139                1732.0              2098.0   \n",
       "5  5222022                777                 539.0               665.0   \n",
       "6  5232022               3381                   0.0                 0.0   \n",
       "\n",
       "   unmatched_job_counts  data quality_complete  data quality_match  \n",
       "0                  2607              42.323174           50.028752  \n",
       "1                   630              48.694316           51.612903  \n",
       "2                   193              84.572342           94.857447  \n",
       "3                    20              92.497200           99.552072  \n",
       "4                    41              80.972417           98.083216  \n",
       "5                   112              69.369369           85.585586  \n",
       "6                  3381               0.000000            0.000000  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many complete job listings were we able to collect each day? How would you interpret this result with respect to data quality? \n",
    "# Does it mean that our data collection strategy is flawed and thus introduces non-random sampling biases?\n",
    "\n",
    "total_date=job_urls_merge.groupby('date').agg({'lnks_link': 'count'}).rename({'lnks_link':'total job listing'}, axis=1).reset_index()\n",
    "c_list=job_urls_merge.columns.to_list()\n",
    "job_complete=job_urls_merge.dropna(subset=c_list).groupby('date').agg({'lnks_link': 'count'}).rename({'lnks_link':'complete job listing'}, axis=1).reset_index()\n",
    "job_dm=job_d[job_d.result=='Matched'].rename({'job_counts': 'matched_job_counts'}, axis=1)\n",
    "job_dnm=job_d[job_d.result=='Not Matched'].rename({'job_counts': 'unmatched_job_counts'}, axis=1)\n",
    "\n",
    "for item in job_complete, job_dm, job_dnm:\n",
    "    total_date=total_date.merge(item, how='left', on='date')\n",
    "total_date=total_date.drop(columns=['result_x', 'result_y']).replace(np.NaN, 0)\n",
    "total_date['data quality_complete']=total_date['complete job listing']/total_date['total job listing']*100\n",
    "total_date['data quality_match']=total_date['matched_job_counts']/total_date['total job listing']*100    \n",
    "total_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee24e4-4c1a-43fa-ac57-a76aeab85b16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The 'complete job listing' column in the data frame shows the number of completed job lists by each day.\n",
    "In terms of data quality, I use two measures of data quality: the rate of the number of complete job lists to the total number of job lists and the rate of the number of matched job lists to the total number of job lists.\n",
    "The former refers to the percentage of job lists that have full information in the job_info_data. The results by date are in the column, 'data quality_complete'. The job urls collected on May 20 shows the highest percentage of completed information while the job urls on May 17 and May 18 are less than 50 percent of completed job information. \n",
    "The latter indicates the percentage of job lists that have the information of links matched with job title. The results by date are in the 'data quality_match' column. \n",
    "The problem in the quality of this data is that there is no matched job urls with job information in May 23, 2022. I think the \"job_info_data\" have to be checked again whether it is collected properly. \n",
    "Considering the fact that the amount of job listings on May 23 is more than 3,000, it might cause the selection bias in the sample related to the seasonal or periodical patterns of job posting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a44117-8eed-469b-b7df-bab82dca8d5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Bonus Task 1.\n",
    "Using the merged dataset from Task 3, extract state information for each individual job based on the “company_location” column. Aggregate them by state, and create a state-level choropleth map to visualize the spatial distribution of compliance jobs. The map should be colored based on the total number of compliance jobs in each state. The boundaries of US states in geojson can be found here. Interpret the results. [You will get three extra points if you are able to finish this bonus task.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed4ada-1b7b-4b93-9c6f-dfae0e02a0be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:compsoc] *",
   "language": "python",
   "name": "conda-env-compsoc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}